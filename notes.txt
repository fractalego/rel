1) The execution time is proportional to the number of rules. Make rules parallel at model.py level
2) You should execute train_a_single_path() one epoch at a time, so that you can stop when the best index matches

The problem here is that the paths and rules are being trained at the same time. There is no way to save
the original vectors and start from there. How do you save old copies of pytorch vectors?


Time:
* 1 goal/data graph, 1 rule (the right one): 30sec
* 1 goal/data graph, 10 rules (with 4 right ones): ~20min
* 1 goal/data graph, 10 rules (with 4 right ones), manual (#) pre-selection: 115sec


Steps:
1) create all the paths with all the rules (where * is an all-match)
    * Are gradient rules matched on all the predicates as an all match?
        * So it seems! Every gradient rule is automatically matched and inserted in the list!!
        * Maybe you should do the matching instead.
        * Maybe with some special condition about * predicates

2) each rule can be applied in different ways
* 20dim embeddings seem to be too low for ~60 relations.

* unconnected graphs seem to confuse the system. actually, every graph bigger than the match seem to confuse the system.
  The edges are trained correctly though. Maybe if the edges could sort of attach to the target and break the symmetry it would work.


If data < rules:
    ### Why does it not predict with all the rules?
    ### It probably means there are a few competing outputs. Use them all ?

if cannot train:
    ### MORE RULES ARE NEEDED, LOOK AT DATA 3

    ### Is shift in range of 10 enough?



The longes sentence has 70 nodes: in utils.py you should set > 70

Training
    * dgt.fit(epochs=20, step=1e-2, relaxation_epochs=200, relaxation_step=1e-3)
      50 facts
      only produces two rule with 0 recall

    * dgt.fit(epochs=20, step=1e-2, relaxation_epochs=100, relaxation_step=1e-2)
      10 facts
      two rules with 20% recall.
      I suspect the other facts are not learned from because there is no correct hypothesis

    * dgt.fit(epochs=20, step=1e-2, relaxation_epochs=200, relaxation_step=1e-2)
      10 facts
      40% recall.
      I had previously added a "x *(v), *(v, a4), *(a4)"
      There was a problem with the length of _max_items_size

    * dgt.fit(epochs=10, step=1e-2, relaxation_epochs=200, relaxation_step=1e-2)
      10 facts
      13 rules with 40% recall.
      Very slow, I suspect _max_items_size of 30 is the culprit (Maybe you can play with this to speed it up)


     #### TRY TO USE TORCH BOTTLENECK!!